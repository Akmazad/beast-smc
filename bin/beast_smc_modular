#!/usr/bin/env python3
# Author: Aaron Darling
# Author: Mathieu Fourment
# TODO: PEPify with docstrings
#
import sys
import argparse
import os
import math
import xml.etree.ElementTree as ET
from shutil import copyfile, rmtree, copy2
import numpy as np
from functools import partial
import zipfile


def resample_multinomial(weights):
    w = np.exp(weights)
    w /= sum(w)
    res = np.random.multinomial(len(weights), w)
    pIndexes = []
    for idx, count in enumerate(res):
        if count > 0:
            pIndexes.extend([idx]*count)
    assert len(pIndexes) == len(weights)
    return pIndexes


def resample_systematic(weights, stratified=False):
    # Procedure for stratified sampling
    # See Appendix of Kitagawa 1996, http://www.jstor.org/stable/1390750,
    # or p.290 of the Doucet et al book, an image of which is at:
    # http://cl.ly/image/200T0y473k1d/stratified_resampling.jpg

    w = np.exp(weights)
    w /= sum(w)

    cum = 0
    r = np.random.random_sample() * 1.0/len(weights)

    uCount = np.zeros(len(weights), dtype=np.int)
    j = 0
    for i, weight in enumerate(w):
        cum += weight
        while cum > ((float(j) / len(w)) + r):
            uCount[i] += 1
            j += 1

            # The only difference between stratified and systematic resampling
            # is whether a new random variable is drawn for each partition of
            # the (0, 1] interval.
            if stratified:
                r = np.random.random_sample() * 1.0/len(weights)

    pIndexes = []
    for idx, count in enumerate(uCount):
        if count > 0:
            pIndexes.extend([idx]*count)
    assert len(pIndexes) == len(weights)

    return pIndexes


def ESS(weights):
    sum = 0
    sumsq = 0
    for w in weights:
        sum += math.exp(w)
    for w in weights:
        sumsq += math.exp(2.0 * w)
    return math.exp(-math.log(sumsq) + 2 * math.log(sum))


def parse_taxon_list(xml_path):
    xml_tree = ET.parse(xml_path)
    taxa = xml_tree.find('taxa')
    taxa_set = set()
    for taxon in taxa.findall('taxon'):
        taxa_set.add(taxon.attrib['id'])
    return taxa_set


def parse_ll(checkpoint_str):
    for line in checkpoint_str.splitlines():
        if line.startswith('lnL'):
            ll = line.split('\t')
            if len(ll) == 2:
                return float(ll[1]), 0
            else:
                return float(ll[1]), float(ll[2])
    return 0 #TODO: throw an error here


def add_subtree_leap_operator(parent, size, weight, treeModel, taxa=None):
    if taxa is not None:
        op = ET.SubElement(parent, 'tipLeap', {'size': size, 'weight': weight})
        ET.SubElement(op, 'treeModel', {'idref': treeModel})
        taxaElement = ET.SubElement(op, 'taxa')
        if isinstance(taxa, str):
            ET.SubElement(taxaElement, 'taxon', {'idref': taxa})
        else:
            for taxon in list(taxa):
                ET.SubElement(taxaElement, 'taxon', {'idref': taxon})
    else:
        ET.SubElement(parent, 'subtreeLeap', {'size': size, 'weight': weight})


def prep_smc():
    # set up particles from MCMC checkpoints
    taxon = args.taxon
    added_taxa = {taxon}
    instances = math.ceil(p_count / args.ppi)

    zip_mode = zipfile.is_zipfile(args.checkpoint_dir)

    # first iteration checkpoint_dir is a single zip file containing checkpoint files
    if zip_mode:
        input_zip = zipfile.ZipFile(args.checkpoint_dir)
        cp_files = [info.filename for info in input_zip.infolist()
                    if info.filename.startswith(args.state_stem) and not info.filename.endswith('/')]

        index_cp_file = 0
        for i in range(instances):
            group_dir = os.path.join(args.output, "group."+str(i))
            os.mkdir(group_dir)
            group_zip = zipfile.ZipFile(os.path.join(group_dir, 'checkpt.zip'), mode='w',
                                        compression=zipfile.ZIP_DEFLATED, compresslevel=8)
            end_part = min(p_count, (i + 1) * args.ppi)
            for j in range(i * args.ppi, end_part):
                # checkpoint files are recycled when the number of particles requested is higher than the number
                # of checkpoint files
                if index_cp_file >= len(cp_files):
                    index_cp_file = 0
                with input_zip.open(cp_files[index_cp_file], 'r') as ckpnt_file:
                    group_zip.writestr('{}_{}.ckpnt'.format(args.state_stem, j), ckpnt_file.read())
                index_cp_file += 1
            group_zip.close()
        input_zip.close()
    else:
        for f in os.listdir(args.checkpoint_dir):
            if f.startswith('group'):
                group_dir = os.path.join(args.output, f)
                os.mkdir(group_dir)
                for f2 in os.listdir(os.path.join(args.checkpoint_dir, f)):
                    if f2.endswith('.zip'):
                        copy2(os.path.join(args.checkpoint_dir, f, f2), os.path.join(group_dir, 'checkpt.zip'))

    for i in range(instances):
        group_dir = os.path.join(args.output, "group."+str(i))
        ptrees_file = os.path.join(group_dir, 'particle.trees')
        plog_file = os.path.join(group_dir, 'particle.log')
        pxml_file = os.path.join(group_dir, 'beast.xml')

        # remove any taxa and associated sequences that we are not yet ready to add
        remove_taxa = taxa_to_add.difference(added_taxa)
        pxml_tree = ET.parse(args.new_xml)

        # from the second iteration the state files contain tipLeap operator but not new_xml
        operatorsElement = pxml_tree.find('operators')
        if use_tip_leap and operatorsElement.find('tipLeap') is None:
            if zip_mode:
                group_zip = zipfile.ZipFile(os.path.join(group_dir, 'checkpt.zip'), mode='r')
            else:
                group_zip = zipfile.ZipFile(os.path.join(args.checkpoint_dir, 'group.'+str(i), 'checkpt-resampled.out.zip'), mode='r')
            first_ckpnt_file = args.state_stem + '_' + str(i * args.ppi) + '.ckpnt'
            #first_ckpnt_file = os.path.join(group_dir, args.state_stem + '_' + str(i * args.ppi) + '.ckpnt')
            with group_zip.open(first_ckpnt_file, 'r') as fp:
                if 'tipLeap' in str(fp.read()):
                    treeModelElement = pxml_tree.find('treeModel')
                    # TODO: initialize with a better weight
                    # In this case we don't care about the size value
                    # since it is going to be overwritten by the state file
                    add_subtree_leap_operator(operatorsElement, '1', '1', treeModelElement.attrib['id'], taxon)
            group_zip.close()
        taxa = pxml_tree.find('taxa')
        for t in taxa.findall('taxon'):
            if t.attrib['id'] in remove_taxa:
                taxa.remove(t)
        alignment = pxml_tree.find('alignment')
        for s in alignment.findall('sequence'):
            if s[0].attrib['idref'] in remove_taxa: # gets the taxon for the sequence
                alignment.remove(s)

        # set the desired mcmc chain length
        mcmc = pxml_tree.find('mcmc')
        mcmc.attrib['chainLength']=str(args.mcmc_iters)

        # don't log to files
        for p in mcmc.findall('log/..'):
            for l in p.findall('log'):
                if l.attrib['id']=='fileLog':
                    p.remove(l)

            for lt in p.findall('logTree'):
                p.remove(lt)

        # TODO: add the targeted operator to the new XML
        pxml_tree.write(pxml_file)


parser = argparse.ArgumentParser(description='Add sequences to a BEAST run using SMC.')
parser.add_argument('--mode', metavar='mode', type=str,
                    help='[all|prep|filter] mode to run in',
                    default='all')
parser.add_argument('--checkpoint_dir', metavar='cpdir', type=str,
                    help='the location of the BEAST checkpoint directory',
                    required=False)
parser.add_argument('--original_xml', type=str, help='The original BEAST XML file',
                    required=False)
parser.add_argument('--new_xml', type=str, help='The BEAST XML file with new sequences',
                    required=False)
parser.add_argument('--particle_dir', metavar='pdir', type=str,
                    help='the location of the updated particle directory',
                    required=False)
parser.add_argument('--particles', type=int,
                    help='The number of particles to use', default=10000)
parser.add_argument('--ppi', type=int,
                    help='The number of particles to update per BEAST instance', default=100)
parser.add_argument('--mcmc_iters', type=int,
                    help='The number of MCMC iterations after each' \
                    ' sequence addition', default=500)
parser.add_argument('--nxfconfig', type=str, help='A nextflow configuration file')
parser.add_argument('--output', type=str, help='A destination directory for output',
                    default='output')
parser.add_argument('--threshold', type=float, help='Resampling threshold. If threshold is positive and less than 1'
                                                    'then resampling is triggered when ESS < threshold*particles. If'
                                                    ' threshold > 1 then resampling is triggered when threshold < ESS.',
                    default=0.5)
parser.add_argument('--sampling', default='multinomial', choices=['multinomial', 'stratified', 'systematic'],
                    help='Sampling method')
parser.add_argument('--weights', type=str, help='Weights at previous iteration',
                    required=False)
parser.add_argument('--state_stem', type=str, default='beast_state', help='Stem of state files from BEAST')
parser.add_argument('--taxon', type=str, help='New taxon', required=False)
parser.add_argument('--overwrite', help='Overwrite output folder', action='store_true')
parser.add_argument('--update_choice', default='F84Distance',
                    choices=['JC69Distance', 'F84Distance', 'Simple', 'stochastic'],
                    help='Update mechanism of CheckPointUpdaterApp')
args = parser.parse_args()

if os.path.lexists(args.output) and args.overwrite:
    rmtree(args.output)

resample_stratified = partial(resample_systematic, stratified=True)
resample = {
    'systematic': resample_systematic,
    'stratified': resample_stratified,
    'multinomial': resample_multinomial,
}

p_count = args.particles

threshold = args.threshold
if args.threshold < 1:
    threshold = args.threshold*p_count

use_tip_leap = False

if args.mode == 'all':
    # this mode runs the nextflow workflow once per new taxon
    orig_xml_path = os.path.abspath(args.original_xml)
    new_xml_path = os.path.abspath(args.new_xml)
    cur_ckpnts = os.path.abspath(args.checkpoint_dir)
    zip_mode = zipfile.is_zipfile(cur_ckpnts)
    out_path = os.path.abspath(args.output)
    # parse the original BEAST XML to get a taxon list
    orig_taxa_set = parse_taxon_list(orig_xml_path)
    # parse the new BEAST XML to find the new taxa
    new_taxa_set = parse_taxon_list(new_xml_path)
    taxa_to_add = new_taxa_set.difference(orig_taxa_set)
    os.mkdir(args.output)
    os.mkdir(os.path.join(out_path, 'iter.0'))

    with open(os.path.join(out_path, 'iter.0', 'weights.csv'), 'w') as fp:
        fp.write(','.join(['0']*args.particles))

    for iter, taxon in enumerate(taxa_to_add):
        iter_dir = os.path.join(out_path, 'iter.' + str(iter))
        weights_file = os.path.join(iter_dir, 'weights.csv')
        if iter > 0:
            os.mkdir(iter_dir)
            copy2(os.path.join(out_path, 'iter.{}'.format(iter-1), 'weights-resampled.csv'),
                weights_file)
        nf_cmd = 'beast_smc.nf '
        if args.nxfconfig is not None:
            nf_cmd += ' -c '+args.nxfconfig
        nf_cmd += ' --original_xml=' + orig_xml_path + \
            ' --new_xml=' + new_xml_path + \
            ' --ckpnt=' + cur_ckpnts + \
            ' --out_dir=' + iter_dir + \
            ' --weights=' + weights_file + \
            ' --threshold=' + str(args.threshold) + \
            ' --particles=' + str(args.particles) + \
            ' --particles_per_instance=' + str(args.ppi) + \
            ' --stem={}'.format(args.state_stem) + \
            ' --taxon="{}"'.format(taxon) + \
            ' --update_choice={}'.format(args.update_choice)
        print(nf_cmd)
        x = os.system(nf_cmd)
        if x != 0:
            exit(x)
        iter += 1
        orig_xml_path=os.path.join(iter_dir, 'beast.xml')
        cur_ckpnts=os.path.join(iter_dir)


if args.mode == 'prep':
    orig_xml_path = os.path.abspath(args.original_xml)
    new_xml_path = os.path.abspath(args.new_xml)

    # parse the original BEAST XML to get a taxon list
    orig_taxa_set = parse_taxon_list(orig_xml_path)

    # parse the new BEAST XML to find the new taxa
    new_taxa_set = parse_taxon_list(new_xml_path)

    taxa_to_add = new_taxa_set.difference(orig_taxa_set)

    prep_smc()

if args.mode == 'filter':
    prev_ll = np.zeros(p_count)
    new_ll = np.zeros(p_count)
    proposal_lp = np.zeros(p_count)
    with open(args.weights) as fp:
        line = fp.read()
        weights = np.array([float(x) for x in line.split(',')])

    instances = math.ceil(p_count / args.ppi)
    i = 0
    for instance in range(instances):
        group_zip = zipfile.ZipFile(os.path.join(args.particle_dir, "group." + str(instance), 'checkpt.zip'))
        new_group_zip = zipfile.ZipFile(
            os.path.join(args.particle_dir, "group." + str(instance), 'checkpt-updated.zip'))
        for info in group_zip.infolist():
            with group_zip.open(info.filename, 'r') as f:
                prev_ll[i], _ = parse_ll(f.read().decode("utf-8"))
            with new_group_zip.open(info.filename, 'r') as f:
                new_ll[i], proposal_lp[i] = parse_ll(f.read().decode("utf-8"))

            weights[i] += new_ll[i] - prev_ll[i] - proposal_lp[i]
            i += 1
        group_zip.close()
        new_group_zip.close()

    # compute the ESS
    ess = ESS(weights)
    print("ESS: " + str(ess))
    print("weights are:")
    print(weights)
    print(new_ll)
    print(prev_ll)
    print(proposal_lp)

    # optionally do resampling
    if ess < threshold:
        print("Resampling")
        ckpnts = resample[args.sampling](weights)
        print(ckpnts)
        weights = [0]*p_count
        prev_ll = new_ll[ckpnts]
    else:
        prev_ll = new_ll
        ckpnts = range(p_count)

    with open('weights-resampled.csv', 'w') as fp:
        fp.write(','.join([str(x) for x in weights]))

    instances = math.ceil(p_count / args.ppi)

    pxml_tree = ET.parse(args.new_xml)
    operatorsElement = pxml_tree.find('operators')
    tipLeapOperator = operatorsElement.find('tipLeap')
    # only called during the first iteration
    if use_tip_leap and tipLeapOperator is None:
        treeModelElement = pxml_tree.find('treeModel')
        # size value is going to change from the state file
        # TODO: initialize with a better weight
        add_subtree_leap_operator(operatorsElement, '2', '1', treeModelElement.attrib['id'], args.taxon)

        for i in range(instances):
            group_dir = os.path.join(args.output, "group." + str(i))
            # os.mkdir(group_dir)
            pxml_tree.write(os.path.join(group_dir, os.path.basename(args.new_xml)))

    group_zip = None
    group_id_current = -1
    resampled_group_zip = None
    resampled_group_id_current = -1
    for i, ckpnt in enumerate(sorted(ckpnts)):
        group_id = math.floor(ckpnt / args.ppi)
        resampled_group_id = math.floor(i / args.ppi)
        if group_id != group_id_current:
            if group_zip is not None:
                group_zip.close()
            group_dir = os.path.join(args.output, "group." + str(group_id))
            group_zip = zipfile.ZipFile(os.path.join(group_dir, 'checkpt-updated.zip'), mode='r')
            group_id_current = group_id
        if resampled_group_id != resampled_group_id_current:
            if resampled_group_zip is not None:
                resampled_group_zip.close()
            resampled_group_zip = zipfile.ZipFile(
                os.path.join(args.output, "group." + str(resampled_group_id), 'checkpt-resampled.zip'), mode='w',
                compression=zipfile.ZIP_DEFLATED, compresslevel=8)
            resampled_group_id_current = resampled_group_id

        # read particle from *updated.zip and write particle to *resampled.zip
        with group_zip.open('{}_{}.ckpnt'.format(args.state_stem, ckpnt)) as ckpnt_file:
            content = ckpnt_file.read().decode('utf-8').splitlines()
            if use_tip_leap and tipLeapOperator is None:
                op = False
                for k, line in enumerate(content):
                    if line.startswith('operator'):
                        op = True
                    elif not line.startswith('operatjoinor') and op:
                        # TODO: initialize with appropriate size (last value)
                        content.insert(k, 'operator\ttipLeap({})\t0\t0\t0.1'.format(treeModelElement.attrib['id']))
                        break
            resampled_group_zip.writestr('{}_{}.ckpnt'.format(args.state_stem, i), '\n'.join(content))
    group_zip.close()
    resampled_group_zip.close()
